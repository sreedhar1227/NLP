{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count_vectorizer_sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pFinCNTIFY5k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0L-K0GiFY5n",
    "outputId": "f8c2ae48-f627-4648-d1d2-75d503d0b060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'overall', 'verified', 'reviewTime', 'reviewerID', 'asin',\n",
      "       'style', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime',\n",
      "       'vote', 'image'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 1000\n",
    "chunks = pd.read_csv(r\"D:\\Generative AI\\aws_review_sofware_dataset.csv\", sep=',', chunksize=chunk_size)\n",
    "\n",
    "# Get the first chunk and access its columns\n",
    "df = next(chunks)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjBXGrN4FY5n",
    "outputId": "b6c47821-d708-42a6-ef13-925a65af4724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'overall', 'verified', 'reviewTime', 'reviewerID', 'asin',\n",
       "       'style', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime',\n",
       "       'vote', 'image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-PQQNN6cFY5o"
   },
   "outputs": [],
   "source": [
    "df[\"words\"]=\"default value\"\n",
    "df[\"sentences\"]=\"default value\"\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]): \n",
    "    df.at[i,\"words\"]= list(\"\")\n",
    "    df.at[i,\"sentences\"] = list(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: This initializes two new columns, words and sentences, in the DataFrame df. \n",
    "\n",
    "Every cell in these columns is initially set to the string \"default value\".\n",
    "\n",
    "This loop iterates over the rows of the DataFrame. df.shape[0] gives the number of rows in the DataFrame.\n",
    "\n",
    "This modifies the values of the words and sentences columns for the row at index i.\n",
    "\n",
    "df.at[i, \"words\"]: Accesses the value in the words column at row index i.\n",
    "\n",
    "list(\"\"): Converts an empty string (\"\") into a list. The result is an empty list [].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hyfzgANXFY5o"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The line from nltk.tokenize import sent_tokenize imports the sent_tokenize function from the Natural Language Toolkit (NLTK) library. This function is used to split a given text into sentences.\n",
    "# What Does sent_tokenize Do?\n",
    "sent_tokenize:\n",
    "\n",
    "Breaks a paragraph or block of text into a list of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ziDec2kGqGZ",
    "outputId": "f70b6464-6d92-48a9-81e7-4ba1c24647d7"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command nltk.download('punkt') is used to download the Punkt tokenizer models in the NLTK library. These models are essential for tokenizing text into sentences or words and are pre-trained to recognize sentence boundaries in various languages.\n",
    "\n",
    "What is Punkt?\n",
    "\n",
    "Punkt is an unsupervised machine learning algorithm for tokenizing text. It was developed by Martin Kiss and is widely used for identifying:\n",
    "\n",
    "1)Sentence boundaries (sent_tokenize).\n",
    "\n",
    "2)Word boundaries (word_tokenize).\n",
    "\n",
    "Purpose of nltk.download('punkt')\n",
    "\n",
    "It downloads the data files required for sentence and word tokenization.\n",
    "\n",
    "These files contain pre-trained models that help recognize:\n",
    "\n",
    "Sentence-ending punctuation (e.g., ., !, ?).\n",
    "\n",
    "Abbreviations (e.g., Dr., e.g., i.e.).\n",
    "\n",
    "Edge cases like quotes or ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e7GBm2-dFY5o"
   },
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    l1= sent_tokenize(str(df.loc[i,\"reviewText\"]))\n",
    "    df.at[i,\"sentences\"]=l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates through each row in a DataFrame df, extracts the text from the column \"reviewText\", tokenizes it into sentences using sent_tokenize, and assigns the resulting list of sentences to the column \"sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2A4_sKfGz4H",
    "outputId": "68605f3a-f82c-4e8c-eae8-0cff83ae0c99"
   },
   "outputs": [],
   "source": [
    "#!pip install pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AM86jWs9HCeg",
    "outputId": "41acd97f-8a36-48d6-d109-d97d846a4e40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Sreedhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Averaged Perceptron Tagger?\n",
    "\n",
    "Purpose: It is a machine learning-based POS tagger provided by NLTK that assigns parts of speech (e.g., noun, verb, adjective) to each word in a sentence.\n",
    "\n",
    "Usage: It works well for English text and is pre-trained on annotated corpora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ANmiKLnvFY5o"
   },
   "outputs": [],
   "source": [
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line from pywsd.utils import lemmatize_sentence imports the lemmatize_sentence function from the PyWSD (Python Word Sense Disambiguation) library. This function is used for lemmatizing a sentence, which means reducing each word to its base or root form, while considering its part of speech.\n",
    "\n",
    "What is Lemmatization?\n",
    "\n",
    "Lemmatization reduces words to their dictionary (base) form, known as the lemma.\n",
    "\n",
    "Unlike stemming, lemmatization is more context-aware and uses a vocabulary and morphological analysis.\n",
    "\n",
    "Example: \"running\" → \"run\"\n",
    "\n",
    "Example: \"better\" → \"good\" (Lemmatization considers meaning and context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-G5GDiJRHM1U",
    "outputId": "01ee2073-4dfb-46a3-f34e-e8391a9aa5d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sreedhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command nltk.download('wordnet') downloads the WordNet lexical database in NLTK. This resource is essential for various Natural Language Processing (NLP) tasks, including lemmatization and word sense disambiguation.\n",
    "\n",
    "#### What is WordNet?\n",
    "WordNet is a large, freely available lexical database of the English language.\n",
    "\n",
    "It groups words into sets of synonyms called synsets and provides short definitions and usage examples.\n",
    "#### WordNet helps with:\n",
    "Understanding word meanings (semantics).\n",
    "\n",
    "Finding synonyms, antonyms, and hyponyms.\n",
    "\n",
    "Lemmatization: Reducing a word to its dictionary form based on its meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "85rhNEkqFY5p"
   },
   "outputs": [],
   "source": [
    "for k in range(df.shape[0]):\n",
    "    df.at[k,\"words\"]=list(\"\")\n",
    "    for j in range(len(df.loc[k,\"sentences\"])):\n",
    "        df.at[k,\"words\"].extend(lemmatize_sentence(df.loc[k,\"sentences\"][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "10aGrWfgFY5p"
   },
   "outputs": [],
   "source": [
    "df[\"words_sentences\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "n8uDprf8FY5q"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "for k in range(df.shape[0]):\n",
    "    df.loc[k,\"words_sentences\"]=functools.reduce(lambda a,b:( str(a)+str(\" \")+str(b)),df.loc[k,\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "k9MWplkhFY5q"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import  CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Z3KOHSjbFY5r"
   },
   "outputs": [],
   "source": [
    "df1=df\n",
    "\n",
    "no_features = 1000\n",
    "tf_vectorizer = CountVectorizer( max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df1.words_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fg1I7mxpFY5r"
   },
   "outputs": [],
   "source": [
    "df_x=pd.DataFrame(tf.A, columns=tf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cdBU7dRiFY5r"
   },
   "outputs": [],
   "source": [
    "df_y=df[\"verified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uFIZvcPmFY5r"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Qy7hCAxEFY5s"
   },
   "outputs": [],
   "source": [
    "\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "R0gmj5TQFY5s"
   },
   "outputs": [],
   "source": [
    "df_y_1=pd.DataFrame(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "E1EqLWEKFY5s"
   },
   "outputs": [],
   "source": [
    "df_y_enc=df_y_1.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvUotomLFY5s",
    "outputId": "5f8d41f1-c6be-4113-efe6-13314b030a9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['verified'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dSt07HY8FY5t",
    "outputId": "857e69cc-2769-4713-b884-8cbd563d26a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verified\n",
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZ1tpdWQF6nH",
    "outputId": "336965c8-6964-4347-ab26-c07559cb4ebb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sreedhar\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(df_x,df_y_enc)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_rf = rf.score(df_x,df_y_enc)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoHxAIzTFyOr",
    "outputId": "e1670543-c0dc-42d9-83ca-7daf9da62cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 89.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sreedhar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(df_x,df_y_enc)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_nb = nb.score(df_x,df_y_enc)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "LI-hQqWMFY5t"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "f6zkNMZYFY5t"
   },
   "outputs": [],
   "source": [
    "GBC=GradientBoostingClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_A5N7TYFY5t",
    "outputId": "b47f597d-6c6f-4098-dd05-30dd1f2a3628"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sreedhar\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "gb_c=GBC.fit(df_x,df_y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OK_EMR3IFY5u",
    "outputId": "5d743957-88e5-4e08-a57c-46e7976fcce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc_score: 96.60%\n"
     ]
    }
   ],
   "source": [
    "gbc_score=GBC.score(df_x,df_y_enc)\n",
    "print(f\"gbc_score: {gbc_score* 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zsUzK6zFY5u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
