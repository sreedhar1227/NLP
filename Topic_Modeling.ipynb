{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRCQiL1Q8Tid"
      },
      "outputs": [],
      "source": [
        "#doc1 = \"I want to watch a movie this weekend\"\n",
        "\n",
        "#doc2 = \"I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton\n",
        "\n",
        "#doc3 = \"I don't watch cricket. Netflix and Amazon Prime have very good movies to watch\n",
        "\n",
        "#doc4 = \"Movies are a nice way to chill however, this time I would like to paint and read some good books. It's been so long!\n",
        "\n",
        "#doc5 = This blueberry milkshake is so good! Try reading Dr. Joe Dispenza's books. His work is such a game-changer! His books helped to learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\n",
        "\n",
        "#doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
        "\n",
        "#doc3 =\"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
        "\n",
        "#doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
        "\n",
        "#doc5 = \"Health experts say that Sugar is not good for your lifestyle.\""
      ],
      "metadata": {
        "id": "b8lWw39t9Gz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = 'Playing cricket is good for health'\n",
        "\n",
        "doc2 = 'Virat is a very famous indian cricketer'\n",
        "\n",
        "doc3 = 'health is wealth'\n",
        "\n",
        "doc4 = 'Cricket is very popular in India'\n",
        "\n",
        "doc5 = 'apple is good for health'\n",
        "\n",
        "doc6 = 'Doing exercise is neccessary for good health'\n",
        "\n",
        "doc7 = 'Now a days Everyone paying attention towards there health'"
      ],
      "metadata": {
        "id": "a6uTjjwt9P6B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile documents\n",
        "doc_complete= [doc1, doc2, doc3, doc4, doc5, doc6,doc7]"
      ],
      "metadata": {
        "id": "skfeZhM_9d96"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JyRAUDT9foN",
        "outputId": "006333b5-dffc-42cc-a94c-2e3ebed0636b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(doc):\n",
        "  stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "  punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "  normalized = \" \".join(lemma.lemmatize(word,pos='n') for word in punc_free.split())\n",
        "  return normalized\n",
        "\n",
        "doc_clean = [clean(doc).split() for doc in doc_complete]"
      ],
      "metadata": {
        "id": "9pJP5j04-qZZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_WHTHlH_UvV",
        "outputId": "5545e9c6-ca7e-4a77-f4ae-2caa28e64f45"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['playing', 'cricket', 'good', 'health'],\n",
              " ['virat', 'famous', 'indian', 'cricketer'],\n",
              " ['health', 'wealth'],\n",
              " ['cricket', 'popular', 'india'],\n",
              " ['apple', 'good', 'health'],\n",
              " ['exercise', 'neccessary', 'good', 'health'],\n",
              " ['day', 'everyone', 'paying', 'attention', 'towards', 'health']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert a corpus into a document-term matrix.\n",
        "#Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora"
      ],
      "metadata": {
        "id": "u-Rf9yNn_1Up"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
        "dictionary = corpora. Dictionary(doc_clean)"
      ],
      "metadata": {
        "id": "nVufX-OtBMOL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ0KqI6_BUQi",
        "outputId": "6e24062f-7615-485f-ba8b-179a1b22c495"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary<19 unique tokens: ['cricket', 'good', 'health', 'playing', 'cricketer']...>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the integer id\n",
        "print(dictionary.token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ4YbQ5gBXsZ",
        "outputId": "5b746e06-93ec-4712-e4a8-45bda2e19770"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cricket': 0, 'good': 1, 'health': 2, 'playing': 3, 'cricketer': 4, 'famous': 5, 'indian': 6, 'virat': 7, 'wealth': 8, 'india': 9, 'popular': 10, 'apple': 11, 'exercise': 12, 'neccessary': 13, 'attention': 14, 'day': 15, 'everyone': 16, 'paying': 17, 'towards': 18}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow (doc) for doc in doc_clean]"
      ],
      "metadata": {
        "id": "PQgWxuoeBcmd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_term_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVLRODCHB9gJ",
        "outputId": "236dc8ae-10a1-4658-b321-dd9562932efc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (8, 1)], [(0, 1), (9, 1), (10, 1)], [(1, 1), (2, 1), (11, 1)], [(1, 1), (2, 1), (12, 1), (13, 1)], [(2, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
      ],
      "metadata": {
        "id": "1jkpeCojCAcJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_term_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRcO1RbJCmz-",
        "outputId": "7c42ed51-ea67-418e-947b-612b68ba9736"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (8, 1)], [(0, 1), (9, 1), (10, 1)], [(1, 1), (2, 1), (11, 1)], [(1, 1), (2, 1), (12, 1), (13, 1)], [(2, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel"
      ],
      "metadata": {
        "id": "hlfls0oPCpvG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and Trainign LDA model on the document term matrix.\n",
        "\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=2, id2word = dictionary, passes=50)\n",
        "\n",
        "# Passes Number of passes through the corpus during training\n",
        "\n",
        "#id2word= It is used to determine the vocabulary size"
      ],
      "metadata": {
        "id": "yq7xhMuICy2L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the first 10 words belonging to each topic\n",
        "\n",
        "for idx, topic in ldamodel.show_topics(formatted=False, num_words= 4):\n",
        "  print('Topic: {} \\nwords: {}'.format(idx, [w[0] for w in topic]))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bAimA_5Dtr2",
        "outputId": "40eb2003-1515-4443-dc91-4f0c5123f773"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "words: ['famous', 'cricketer', 'indian', 'virat']\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "words: ['health', 'good', 'cricket', 'paying']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SG05ORRiEAM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}